{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75bc6233",
   "metadata": {},
   "source": [
    "## Importing libraries and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38f5bfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import nltk\n",
    "import pyphen\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcc7cdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'C:\\Users\\trash\\BlackCoffer Task\\Input.xlsx - Sheet1.csv')\n",
    "output_format = pd.read_csv(r'C:\\Users\\trash\\BlackCoffer Task\\Output Data Structure.xlsx - Sheet1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d9057ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_stopwords(auditor,currency,dates_numbers,generic,generic_long,geographic,names):\n",
    "    \n",
    "    all_text_files = [auditor,currency,dates_numbers,generic,generic_long,geographic,names]\n",
    "    stopwords_list = []\n",
    "\n",
    "    for i in all_text_files:\n",
    "        \n",
    "        new_data = i.read()\n",
    "\n",
    "        data_into_list = new_data.replace('\\n', ' ').replace('|',' ')\n",
    "        \n",
    "        stopwords_list.append(data_into_list)\n",
    "        \n",
    "        res = \"\".join(stopwords_list)\n",
    "        \n",
    "        # removing all html tags and keeping only alphabets and numbers\n",
    "        text = re.sub(r'http\\S+', '', res)\n",
    "        html_tags = re.sub(r\"<*\\S\\s\\w*>\",\"\",text)\n",
    "        cleaned_text = re.sub('[^A-Za-z0-9]+',' ',html_tags).lower()\n",
    "        \n",
    "        final_words = cleaned_text.split(' ')\n",
    "        \n",
    "        test_list = [i for i in final_words if i]\n",
    "\n",
    "    return set(test_list)\n",
    "\n",
    "    \n",
    "auditor = open(r'C:\\Users\\trash\\BlackCoffer Task\\WebScrapping_BlackCoffer\\drive-download-20230112T062143Z-001\\StopWords\\StopWords_Auditor.txt','r')\n",
    "currency = open(r'C:\\Users\\trash\\BlackCoffer Task\\WebScrapping_BlackCoffer\\drive-download-20230112T062143Z-001\\StopWords\\StopWords_Currencies.txt','r')\n",
    "dates_numbers = open(r'C:\\Users\\trash\\BlackCoffer Task\\WebScrapping_BlackCoffer\\drive-download-20230112T062143Z-001\\StopWords\\StopWords_DatesandNumbers.txt','r')\n",
    "generic = open(r'C:\\Users\\trash\\BlackCoffer Task\\WebScrapping_BlackCoffer\\drive-download-20230112T062143Z-001\\StopWords\\StopWords_Generic.txt','r')\n",
    "generic_long = open(r'C:\\Users\\trash\\BlackCoffer Task\\WebScrapping_BlackCoffer\\drive-download-20230112T062143Z-001\\StopWords\\StopWords_GenericLong.txt','r')\n",
    "geographic = open(r'C:\\Users\\trash\\BlackCoffer Task\\WebScrapping_BlackCoffer\\drive-download-20230112T062143Z-001\\StopWords\\StopWords_Geographic.txt','r')\n",
    "names = open(r'C:\\Users\\trash\\BlackCoffer Task\\WebScrapping_BlackCoffer\\drive-download-20230112T062143Z-001\\StopWords\\StopWords_Names.txt','r')\n",
    "\n",
    "set_of_all_stopwords = all_stopwords(auditor,currency,dates_numbers,generic,generic_long,geographic,names)\n",
    "\n",
    "\n",
    "# Combining all text files which include stopwords from StopWords folder mentioned and combining them to make a single set of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39adc531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positive_words(pos_words):\n",
    "    \n",
    "    # reading and replacing \\n with empty space and splitting on basis of empty space\n",
    "    start = pos_words.read()\n",
    "    \n",
    "    data_into_list = start.replace('\\n', ' ').split(' ')\n",
    "    \n",
    "    return data_into_list\n",
    "    \n",
    "    \n",
    "pos_words = open(r'C:\\Users\\trash\\BlackCoffer Task\\WebScrapping_BlackCoffer\\drive-download-20230112T062143Z-001\\MasterDictionary\\positive-words.txt','r')\n",
    "\n",
    "list_for_positive_words = positive_words(pos_words)\n",
    "\n",
    "# Reading and creating a list out of positive words mentioned in text file under MasterDictionary Folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd195a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_words(neg_words):\n",
    "    \n",
    "    # reading and replacing \\n with empty space and splitting on basis of empty space\n",
    "    start = neg_words.read()\n",
    "    \n",
    "    data_into_list = start.replace('\\n', ' ').split(' ')\n",
    "    \n",
    "    return data_into_list\n",
    "    \n",
    "    \n",
    "neg_words = open(r'C:\\Users\\trash\\BlackCoffer Task\\WebScrapping_BlackCoffer\\drive-download-20230112T062143Z-001\\MasterDictionary\\negative-words.txt','r')\n",
    "\n",
    "list_for_negative_words = negative_words(neg_words)\n",
    "\n",
    "# Reading and creating a list out of negative words mentioned in text file under MasterDictionary Folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45fcd9bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>https://insights.blackcoffer.com/will-machine-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>146</td>\n",
       "      <td>https://insights.blackcoffer.com/blockchain-fo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>147</td>\n",
       "      <td>https://insights.blackcoffer.com/the-future-of...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>148</td>\n",
       "      <td>https://insights.blackcoffer.com/big-data-anal...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>149</td>\n",
       "      <td>https://insights.blackcoffer.com/business-anal...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>150</td>\n",
       "      <td>https://insights.blackcoffer.com/challenges-an...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     URL_ID                                                URL  \\\n",
       "0        37  https://insights.blackcoffer.com/ai-in-healthc...   \n",
       "1        38  https://insights.blackcoffer.com/what-if-the-c...   \n",
       "2        39  https://insights.blackcoffer.com/what-jobs-wil...   \n",
       "3        40  https://insights.blackcoffer.com/will-machine-...   \n",
       "4        41  https://insights.blackcoffer.com/will-ai-repla...   \n",
       "..      ...                                                ...   \n",
       "109     146  https://insights.blackcoffer.com/blockchain-fo...   \n",
       "110     147  https://insights.blackcoffer.com/the-future-of...   \n",
       "111     148  https://insights.blackcoffer.com/big-data-anal...   \n",
       "112     149  https://insights.blackcoffer.com/business-anal...   \n",
       "113     150  https://insights.blackcoffer.com/challenges-an...   \n",
       "\n",
       "     POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       "0               NaN             NaN             NaN                 NaN   \n",
       "1               NaN             NaN             NaN                 NaN   \n",
       "2               NaN             NaN             NaN                 NaN   \n",
       "3               NaN             NaN             NaN                 NaN   \n",
       "4               NaN             NaN             NaN                 NaN   \n",
       "..              ...             ...             ...                 ...   \n",
       "109             NaN             NaN             NaN                 NaN   \n",
       "110             NaN             NaN             NaN                 NaN   \n",
       "111             NaN             NaN             NaN                 NaN   \n",
       "112             NaN             NaN             NaN                 NaN   \n",
       "113             NaN             NaN             NaN                 NaN   \n",
       "\n",
       "     AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "0                    NaN                          NaN        NaN   \n",
       "1                    NaN                          NaN        NaN   \n",
       "2                    NaN                          NaN        NaN   \n",
       "3                    NaN                          NaN        NaN   \n",
       "4                    NaN                          NaN        NaN   \n",
       "..                   ...                          ...        ...   \n",
       "109                  NaN                          NaN        NaN   \n",
       "110                  NaN                          NaN        NaN   \n",
       "111                  NaN                          NaN        NaN   \n",
       "112                  NaN                          NaN        NaN   \n",
       "113                  NaN                          NaN        NaN   \n",
       "\n",
       "     AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
       "0                                 NaN                 NaN         NaN   \n",
       "1                                 NaN                 NaN         NaN   \n",
       "2                                 NaN                 NaN         NaN   \n",
       "3                                 NaN                 NaN         NaN   \n",
       "4                                 NaN                 NaN         NaN   \n",
       "..                                ...                 ...         ...   \n",
       "109                               NaN                 NaN         NaN   \n",
       "110                               NaN                 NaN         NaN   \n",
       "111                               NaN                 NaN         NaN   \n",
       "112                               NaN                 NaN         NaN   \n",
       "113                               NaN                 NaN         NaN   \n",
       "\n",
       "     SYLLABLE PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       "0                  NaN                NaN              NaN  \n",
       "1                  NaN                NaN              NaN  \n",
       "2                  NaN                NaN              NaN  \n",
       "3                  NaN                NaN              NaN  \n",
       "4                  NaN                NaN              NaN  \n",
       "..                 ...                ...              ...  \n",
       "109                NaN                NaN              NaN  \n",
       "110                NaN                NaN              NaN  \n",
       "111                NaN                NaN              NaN  \n",
       "112                NaN                NaN              NaN  \n",
       "113                NaN                NaN              NaN  \n",
       "\n",
       "[114 rows x 15 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ad81442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>https://insights.blackcoffer.com/will-machine-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>146</td>\n",
       "      <td>https://insights.blackcoffer.com/blockchain-fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>147</td>\n",
       "      <td>https://insights.blackcoffer.com/the-future-of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>148</td>\n",
       "      <td>https://insights.blackcoffer.com/big-data-anal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>149</td>\n",
       "      <td>https://insights.blackcoffer.com/business-anal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>150</td>\n",
       "      <td>https://insights.blackcoffer.com/challenges-an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     URL_ID                                                URL\n",
       "0        37  https://insights.blackcoffer.com/ai-in-healthc...\n",
       "1        38  https://insights.blackcoffer.com/what-if-the-c...\n",
       "2        39  https://insights.blackcoffer.com/what-jobs-wil...\n",
       "3        40  https://insights.blackcoffer.com/will-machine-...\n",
       "4        41  https://insights.blackcoffer.com/will-ai-repla...\n",
       "..      ...                                                ...\n",
       "109     146  https://insights.blackcoffer.com/blockchain-fo...\n",
       "110     147  https://insights.blackcoffer.com/the-future-of...\n",
       "111     148  https://insights.blackcoffer.com/big-data-anal...\n",
       "112     149  https://insights.blackcoffer.com/business-anal...\n",
       "113     150  https://insights.blackcoffer.com/challenges-an...\n",
       "\n",
       "[114 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dc760c",
   "metadata": {},
   "source": [
    "## Making the final function for getting the output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09b34cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_output(data,output_format):\n",
    "    \n",
    "    # Looping through each URL in the dataset\n",
    "    for count,j in enumerate(data['URL']):\n",
    "        url = j \n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.content, 'html5lib') # If this line causes an error, run 'pip install html5lib' or install html5lib\n",
    "\n",
    "        # Getting only the content inside the 'paragraph' and 'title' tag inside the whole html\n",
    "        new_data = '' \n",
    "        yub = []\n",
    "        for new_data in soup.find_all(['p','title']): \n",
    "            yub.append(new_data.get_text())\n",
    "\n",
    "        paragraph = ''\n",
    "        for i in yub:\n",
    "            paragraph = paragraph + ' ' + i\n",
    "\n",
    "        all_is = nltk.tokenize.word_tokenize(paragraph)\n",
    "\n",
    "        out = \" \".join(str(r.lower()) for r in all_is)\n",
    "\n",
    "        whole_paragraph = re.sub('[^A-Za-z]+',' ',out)\n",
    "\n",
    "        phrase = whole_paragraph.split(' ')\n",
    "\n",
    "        list_of_all_words = [i for i in phrase if i]\n",
    "\n",
    "        final_words_before_cleaning = set(list_of_all_words)\n",
    "        \n",
    "        total_no_of_words = len(final_words_before_cleaning)\n",
    "        \n",
    "        \n",
    "        # Getting the dictionary of words after cleaning the words from the stopwords we made a set out of earlier in this code.\n",
    "        def total_words_after_cleaning(final_words_before_cleaning,set_of_all_stopwords):\n",
    "            \n",
    "            final_words_after_cleaning = final_words_before_cleaning - set_of_all_stopwords\n",
    "            \n",
    "            return final_words_after_cleaning\n",
    "    \n",
    "        total_words_after_cleaning = total_words_after_cleaning(final_words_before_cleaning,set_of_all_stopwords)\n",
    "        \n",
    "        \n",
    "        # Getting the number of sentences in that particular article. \n",
    "        def total_no_of_sentences(paragraph):\n",
    "\n",
    "            tokenized_sentences=nltk.sent_tokenize(paragraph)\n",
    "\n",
    "            return len(tokenized_sentences)\n",
    "\n",
    "        total_no_of_sentences = total_no_of_sentences(paragraph)\n",
    "        \n",
    "        \n",
    "        # Calculating the positive score using the formula provided in the function according to the doc file i.e \"Text Analysis.docx\".\n",
    "        def positive_score(total_words_after_cleaning,list_for_positive_words):\n",
    "    \n",
    "            return len(total_words_after_cleaning.intersection(list_for_positive_words))\n",
    "\n",
    "        positive_score = positive_score(total_words_after_cleaning,list_for_positive_words)\n",
    "\n",
    "        \n",
    "        # Calculating the negative score using the formula provided in the function according to the doc file i.e \"Text Analysis.docx\".\n",
    "        def negative_score(total_words_after_cleaning,list_for_negative_words):\n",
    "    \n",
    "            return len(total_words_after_cleaning.intersection(list_for_negative_words))\n",
    "\n",
    "        negative_score = negative_score(total_words_after_cleaning,list_for_negative_words)\n",
    "        \n",
    "        \n",
    "        # Calculating the polarity score using the formula provided in the function according to the doc file i.e \"Text Analysis.docx\".\n",
    "        def polarity_score(positive_score,negative_score):\n",
    "    \n",
    "            Polarity_Score = (positive_score - negative_score)/ ((positive_score + negative_score) + 0.000001 )\n",
    "\n",
    "            return np.round(Polarity_Score,2)\n",
    "\n",
    "        polarity_score = polarity_score(positive_score,negative_score)\n",
    "        \n",
    "        \n",
    "        # Calculating the subjectivity score using the formula provided in the function according to the doc file i.e \"Text Analysis.docx\".\n",
    "        def subjectivity_score(positive_score,negative_score,total_words_after_cleaning):\n",
    "    \n",
    "            Subjectivity_Score = (positive_score + negative_score)/((len(total_words_after_cleaning)) + 0.000001 )\n",
    "\n",
    "            return np.round(Subjectivity_Score,2)\n",
    "\n",
    "        subjectivity_score = subjectivity_score(positive_score,negative_score,total_words_after_cleaning)\n",
    "        \n",
    "        \n",
    "        # Calculating the average sentence length using the formula provided in the function according to the doc file i.e \"Text Analysis.docx\".\n",
    "        def average_sentence_length(total_no_of_words,total_no_of_sentences):\n",
    "    \n",
    "            Avg_Sentence_Length = total_no_of_words / total_no_of_sentences\n",
    "\n",
    "            return np.round(Avg_Sentence_Length,2)\n",
    "\n",
    "        average_sentence_length = average_sentence_length(total_no_of_words,total_no_of_sentences)\n",
    "        \n",
    "        \n",
    "        # Calculating the no. of complex words i.e the words having more than 2 syllables are considered as complex words.\n",
    "        def number_of_complex_words(list_of_all_words):\n",
    "            \n",
    "            \n",
    "            count = 0\n",
    "\n",
    "            dic = pyphen.Pyphen(lang='en')\n",
    "\n",
    "            for i in list_of_all_words:\n",
    "                split_words = dic.inserted(i).split('-')\n",
    "                if len(split_words) > 2:\n",
    "                    count += 1\n",
    "\n",
    "            return count\n",
    "\n",
    "        number_of_complex_words = number_of_complex_words(list_of_all_words)\n",
    "        \n",
    "        \n",
    "        # Calculating the percentage of complex words using the formula provided in doc file i.e \"Text Analysis.docx\".\n",
    "        def percentage_of_complex_words(number_of_complex_words,total_no_of_words):\n",
    "    \n",
    "            Percentage_of_complex_words = number_of_complex_words / total_no_of_words\n",
    "\n",
    "            return np.round(Percentage_of_complex_words,2)\n",
    "\n",
    "        percentage_of_complex_words = percentage_of_complex_words(number_of_complex_words,total_no_of_words)\n",
    "        \n",
    "        \n",
    "        # Calculating the fog index using the formula provided in doc file i.e \"Text Analysis.docx\".\n",
    "        def fog_index(average_sentence_length,percentage_of_complex_words):\n",
    "    \n",
    "            Fog_Index = 0.4 * (average_sentence_length + percentage_of_complex_words)\n",
    "\n",
    "            return np.round(Fog_Index,2)\n",
    "\n",
    "        fog_index = fog_index(average_sentence_length,percentage_of_complex_words)\n",
    "        \n",
    "        \n",
    "        # Calculating the average number of words per sentence using the formula provided in doc file i.e \"Text Analysis.docx\".\n",
    "        def avg_no_of_words_per_sent(total_no_of_words,total_no_of_sentences):\n",
    "    \n",
    "            Avg_no_of_Words_per_sentence = total_no_of_words / total_no_of_sentences\n",
    "\n",
    "            return np.round(Avg_no_of_Words_per_sentence,2)\n",
    "\n",
    "        avg_no_of_words_per_sent = avg_no_of_words_per_sent(total_no_of_words,total_no_of_sentences)\n",
    "        \n",
    "        \n",
    "        # Calculating the word count using the method mentioned in doc file i.e \"Text Analysis.docx\".\n",
    "        def word_count(final_words_before_cleaning, whole_paragraph):\n",
    "    \n",
    "            mystop = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "            rem_stop = []\n",
    "            for i in final_words_before_cleaning:\n",
    "                if i not in mystop:\n",
    "                    rem_stop.append(i)\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            string = \" \".join(str(r.lower()) for r in rem_stop)\n",
    "\n",
    "            result = re.sub(r\"[!\\\"#\\$%&\\'\\(\\)\\*\\+,-\\./:;<=>\\?@\\[\\\\\\]\\^_`{\\|}~]\", \"\", string)\n",
    "\n",
    "            final_list = result.split(' ')\n",
    "\n",
    "            return len(final_list)\n",
    "\n",
    "        word_count = word_count(final_words_before_cleaning, whole_paragraph)\n",
    "        \n",
    "        \n",
    "        # Calculating syllables per word using the formula provided in doc file i.e \"Text Analysis.docx\".\n",
    "        def syllable_per_word(list_of_all_words):\n",
    "    \n",
    "            count = 0\n",
    "\n",
    "            dic = pyphen.Pyphen(lang='en')\n",
    "\n",
    "            for i in list_of_all_words:\n",
    "                split_words = dic.inserted(i).split('-')\n",
    "                count += len(split_words)\n",
    "\n",
    "\n",
    "            return int(np.round(count / len(list_of_all_words)))\n",
    "\n",
    "\n",
    "        approx_syllable_per_word = syllable_per_word(list_of_all_words)\n",
    "        \n",
    "        \n",
    "        # Calculating personal pronouns using the method provided in doc file i.e \"Text Analysis.docx\".\n",
    "        def personal_pronouns(paragraph):\n",
    "       \n",
    "            new_I = re.findall(r\"I\", paragraph)\n",
    "            new_we = re.findall(r\"we\", paragraph)\n",
    "            new_my = re.findall(r\"my\", paragraph)\n",
    "            new_ours = re.findall(r\"ours\", paragraph)\n",
    "            new_us = re.findall(r\"us\", paragraph)  # Case Sensitive Matching so that US or Us as a country does not get counted in this.\n",
    "\n",
    "            no_of_pronouns = len(new_I) + len(new_we) + len(new_my) + len(new_ours) + len(new_us)\n",
    "\n",
    "            return no_of_pronouns\n",
    "\n",
    "        personal_pronouns = personal_pronouns(paragraph)\n",
    "        \n",
    "        \n",
    "        # Calculating the average length of word using method provided in doc file i.e \"Text Analysis.docx\".\n",
    "        def average_word_length(total_words_after_cleaning):\n",
    "    \n",
    "            count = 0\n",
    "            for i in total_words_after_cleaning:\n",
    "                count += len(i)\n",
    "\n",
    "            Average_word_length = count / len(total_words_after_cleaning)\n",
    "\n",
    "            return int(np.round(Average_word_length))\n",
    "\n",
    "        approx_average_word_length = average_word_length(total_words_after_cleaning)\n",
    "        \n",
    "        final_list_of_all = [positive_score,negative_score,polarity_score,subjectivity_score,average_sentence_length,percentage_of_complex_words,fog_index,avg_no_of_words_per_sent,number_of_complex_words,word_count,approx_syllable_per_word,personal_pronouns,approx_average_word_length]\n",
    "        \n",
    "        # Filling up the Values in the all the Columns of output file provided\n",
    "        output_format.loc[count,['POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE',\n",
    "       'SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH',\n",
    "       'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX',\n",
    "       'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT', 'WORD COUNT',\n",
    "       'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH']] = final_list_of_all\n",
    "\n",
    "\n",
    "    return output_format\n",
    "\n",
    "final_dataframe = final_output(data,output_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df9bf7e",
   "metadata": {},
   "source": [
    "## Output file after complete calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aff31fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "      <td>43.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.11</td>\n",
       "      <td>10.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>4.32</td>\n",
       "      <td>10.40</td>\n",
       "      <td>315.0</td>\n",
       "      <td>709.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "      <td>37.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.19</td>\n",
       "      <td>6.85</td>\n",
       "      <td>0.29</td>\n",
       "      <td>2.86</td>\n",
       "      <td>6.85</td>\n",
       "      <td>159.0</td>\n",
       "      <td>460.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
       "      <td>47.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.14</td>\n",
       "      <td>8.11</td>\n",
       "      <td>0.39</td>\n",
       "      <td>3.40</td>\n",
       "      <td>8.11</td>\n",
       "      <td>268.0</td>\n",
       "      <td>612.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>https://insights.blackcoffer.com/will-machine-...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.14</td>\n",
       "      <td>6.33</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.64</td>\n",
       "      <td>6.33</td>\n",
       "      <td>161.0</td>\n",
       "      <td>499.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n",
       "      <td>33.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9.06</td>\n",
       "      <td>0.31</td>\n",
       "      <td>3.75</td>\n",
       "      <td>9.06</td>\n",
       "      <td>218.0</td>\n",
       "      <td>608.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>146</td>\n",
       "      <td>https://insights.blackcoffer.com/blockchain-fo...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.15</td>\n",
       "      <td>10.06</td>\n",
       "      <td>0.24</td>\n",
       "      <td>4.12</td>\n",
       "      <td>10.06</td>\n",
       "      <td>117.0</td>\n",
       "      <td>412.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>147</td>\n",
       "      <td>https://insights.blackcoffer.com/the-future-of...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9.65</td>\n",
       "      <td>0.33</td>\n",
       "      <td>3.99</td>\n",
       "      <td>9.65</td>\n",
       "      <td>154.0</td>\n",
       "      <td>409.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>148</td>\n",
       "      <td>https://insights.blackcoffer.com/big-data-anal...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.15</td>\n",
       "      <td>6.52</td>\n",
       "      <td>0.41</td>\n",
       "      <td>2.77</td>\n",
       "      <td>6.52</td>\n",
       "      <td>174.0</td>\n",
       "      <td>362.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>149</td>\n",
       "      <td>https://insights.blackcoffer.com/business-anal...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.11</td>\n",
       "      <td>11.60</td>\n",
       "      <td>0.40</td>\n",
       "      <td>4.80</td>\n",
       "      <td>11.60</td>\n",
       "      <td>115.0</td>\n",
       "      <td>241.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>150</td>\n",
       "      <td>https://insights.blackcoffer.com/challenges-an...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.18</td>\n",
       "      <td>6.58</td>\n",
       "      <td>0.32</td>\n",
       "      <td>2.76</td>\n",
       "      <td>6.58</td>\n",
       "      <td>135.0</td>\n",
       "      <td>361.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     URL_ID                                                URL  \\\n",
       "0        37  https://insights.blackcoffer.com/ai-in-healthc...   \n",
       "1        38  https://insights.blackcoffer.com/what-if-the-c...   \n",
       "2        39  https://insights.blackcoffer.com/what-jobs-wil...   \n",
       "3        40  https://insights.blackcoffer.com/will-machine-...   \n",
       "4        41  https://insights.blackcoffer.com/will-ai-repla...   \n",
       "..      ...                                                ...   \n",
       "109     146  https://insights.blackcoffer.com/blockchain-fo...   \n",
       "110     147  https://insights.blackcoffer.com/the-future-of...   \n",
       "111     148  https://insights.blackcoffer.com/big-data-anal...   \n",
       "112     149  https://insights.blackcoffer.com/business-anal...   \n",
       "113     150  https://insights.blackcoffer.com/challenges-an...   \n",
       "\n",
       "     POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       "0              43.0            26.0            0.25                0.11   \n",
       "1              37.0            28.0            0.14                0.19   \n",
       "2              47.0            24.0            0.32                0.14   \n",
       "3              32.0            22.0            0.19                0.14   \n",
       "4              33.0            22.0            0.20                0.11   \n",
       "..              ...             ...             ...                 ...   \n",
       "109            21.0            26.0           -0.11                0.15   \n",
       "110            19.0            10.0            0.31                0.09   \n",
       "111            18.0            27.0           -0.20                0.15   \n",
       "112            18.0             5.0            0.57                0.11   \n",
       "113            20.0            33.0           -0.25                0.18   \n",
       "\n",
       "     AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "0                  10.40                         0.40       4.32   \n",
       "1                   6.85                         0.29       2.86   \n",
       "2                   8.11                         0.39       3.40   \n",
       "3                   6.33                         0.28       2.64   \n",
       "4                   9.06                         0.31       3.75   \n",
       "..                   ...                          ...        ...   \n",
       "109                10.06                         0.24       4.12   \n",
       "110                 9.65                         0.33       3.99   \n",
       "111                 6.52                         0.41       2.77   \n",
       "112                11.60                         0.40       4.80   \n",
       "113                 6.58                         0.32       2.76   \n",
       "\n",
       "     AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
       "0                               10.40               315.0       709.0   \n",
       "1                                6.85               159.0       460.0   \n",
       "2                                8.11               268.0       612.0   \n",
       "3                                6.33               161.0       499.0   \n",
       "4                                9.06               218.0       608.0   \n",
       "..                                ...                 ...         ...   \n",
       "109                             10.06               117.0       412.0   \n",
       "110                              9.65               154.0       409.0   \n",
       "111                              6.52               174.0       362.0   \n",
       "112                             11.60               115.0       241.0   \n",
       "113                              6.58               135.0       361.0   \n",
       "\n",
       "     SYLLABLE PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       "0                  2.0               95.0              8.0  \n",
       "1                  1.0               69.0              7.0  \n",
       "2                  2.0               88.0              8.0  \n",
       "3                  1.0              121.0              7.0  \n",
       "4                  2.0               94.0              7.0  \n",
       "..                 ...                ...              ...  \n",
       "109                2.0               49.0              8.0  \n",
       "110                2.0               35.0              7.0  \n",
       "111                2.0               32.0              7.0  \n",
       "112                2.0               20.0              8.0  \n",
       "113                2.0               42.0              7.0  \n",
       "\n",
       "[114 rows x 15 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28072c5a",
   "metadata": {},
   "source": [
    "## Exporting results to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17f5f02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting results to csv file \n",
    "\n",
    "final_dataframe.to_csv(r'C:\\Users\\trash\\BlackCoffer Task\\WebScrapping_BlackCoffer\\Output_Data_Structure.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f80d97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
